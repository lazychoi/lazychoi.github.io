{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cb4a5603-9338-447e-8c7b-d9cffb50f5c8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"이토록 쉬운 머신러닝&딥러닝 입문 3장 파이토치를 활용한 딥러닝\"\n",
    "date: 2022/11/17\n",
    "updated: last-modified\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e48a07-e1a4-40cb-9527-ee0fecef22d1",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "## 시그모이드 함수 사용 이유\n",
    "\n",
    "단위 계단 함수(unit step function)은 불연속이므로 미분할 수 없는 지점이 있어 퍼셉트론 학습에 사용하기에 적합하지 않다. 0과 1 사의의 연속적인 값을 가지는 시그모이드(sigmoid) 함수를 사용한다. p214\n",
    ":::\n",
    "\n",
    ":::{.callout-important}\n",
    "다층 퍼셉트론(multi-layer perceptron)에서는 입력 데이터도 하나의 층으로 간주한다. p217\n",
    ":::\n",
    "\n",
    ":::{.callout-important}\n",
    "## 오차 역전파\n",
    "\n",
    "오차 역전파(back-propagation)는 출력층에서 입력층 방향으로 뒤에서부터 오차를 추적하며 경사 하강법을 이용해 가중치와 편향을 수정하는 수학적 방법이다. 오차 역전파의 수학적 원리는 연쇄 법칙(chain rule)과 편미분(partial derivative)이다. 신경망은 구조가 복잡해서 손실 함수의 기울기를 계산하고 경사 하강법으로 가중치와 편향을 수정하는 과정의 연산량이 많기 때문에 이 연산 과정을 수학적으로 개선한 방법이 오차 역전파다. p218\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b2d2d-4fa6-4e84-988c-ff48df188722",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "## 기울기 소실(gradient vanishing) 문제\n",
    "\n",
    "오차 역전파의 계산 과정에서 층마다 활성화 함수의 기울기를 곱하는데, 활성화 함수로 사용하는 시그모이드 함수의 기울기는 0.3 미만이다. 따라서 은닉층을 지날 때마다 0.3 미만의 값을 반복해서 곱하다 보면 입력층에 가까워졌을 때는 기울기가 0에 가까워진다. 가중치를 수정할 때는 기울기의 크기에 학습률을 곱한 만큼 가중치를 수정하게 되는데, 기울기가 0에 가까우면 기울기의 크기에 학습률은 곱한 결과도 0에 가까우므로 가중치가 거의 수정되지 않아 제대로 학습이 되지 않는다. 이를 해결하기 위해 은닉층 활성화 함수로 시그모이드 함수 대신 렐루 함수를 사용한다. 렐루 함수는 값이 0을 넘을 때는 미분 값이 항상 1이기 때문에 많은 은닉층이 있어도 기울기가 소실되지 않고 잘 전달된다.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89053918-f485-4b55-a3d1-54ea0a1d7dd1",
   "metadata": {},
   "source": [
    "## 활성화 함수\n",
    "\n",
    "| 사용층 | 용도 | 활성화 함수 | 설명 |\n",
    "|:-:|:-:|:-:|-|\n",
    "|은닉층| 기울기 소실 문제를 줄이고 다음 층으로 신호 전달 | 렐루 / 리키 렐루 | 0이하는 모두 0, 0 초과는 항상 1 |\n",
    "|출력층| 이진 분류 | 시그모이드 | 양성일 확률로 따져 양성, 음성 판정 |\n",
    "| | 다중 분류 | 소프트맥스 | 각 클래스에 속할 확률을 따져 분류 |\n",
    "| | 회귀 | - | 노드 값을 그대로 출력 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8bff1-5841-4d5e-843d-465eb0bf9c60",
   "metadata": {},
   "source": [
    "## 손실 함수\n",
    "\n",
    "| 용도 | 손실 함수 |\n",
    "|:-:|-|\n",
    "| 이진 분류 | 이진 크로스 엔트로피 |\n",
    "| 다중 분류 | 크로스 엔트로피 |\n",
    "| 회귀 | 평균 제곱 오차|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f57c67-ef9a-4ea4-bd85-db71517fe3f8",
   "metadata": {},
   "source": [
    "## 옵티마이저(optimizer) - 사용하는 경사 하강법 종류\n",
    "\n",
    "| 옵티마이저 | 특징 |\n",
    "|-|-|\n",
    "| 확률적 경사 하강법(SGD, Stochastic Gradient Descent) | 데이터 샘플을 무작위로 추출하여 일부만 경사 하강법을 사용해 학습 속도 개선|\n",
    "| RMSProp(Root Mean Square Propagation) | 학습률이 커서 최적 가중치를 지나치는 문제 해결<br>손실 함수의 기울기가 크면 학습률이 크고 작으면 학습률이 작아지도록 조절 |\n",
    "| 모멘텀(momentum) | 지역 최솟값에 갇히는 문제를 해결하기 위해<br>손실 함수의 기울기가 0인 여러 지점 중 한 지점에서 멈추지 않고<br>계속 최솟값을 찾도록 관성 개념을 추가 |\n",
    "| 아담(adam) | RMSProp과 모멘텀 기능을 합침 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2b9ea-091c-4c24-adbb-e0b034138513",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "## 텐서(tensor) 자료구조\n",
    "\n",
    "넘파이 배열과 구조적, 기능적으로 거의 비슷하지만 그래픽 카드를 사용해 병렬 연산을 할 수 있다는 차이가 있다. NVIDIA 그래픽 카드가 제공하는 쿠다(cuda) 환경에서 모델을 학습할 수 있다. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80619c1d-b84b-4ca2-82ba-19a16a5b32d7",
   "metadata": {},
   "source": [
    "## 선형 회귀 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef6df8d-4a21-40ba-81da-f7ff8794b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim # 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811be2c9-39c7-45d7-8421-030233563f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_boston()\n",
    "\n",
    "X, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc676925-af1d-4f53-ab66-7b2f0ccd0648",
   "metadata": {},
   "source": [
    "### 특성과 타깃을 텐서 자료구조로 변환\n",
    "\n",
    "- 실수형 텐서: torch.FloatTensor\n",
    "- 정수형 텐서: torch.LongTensor\n",
    "- 파이토치는 타겟을 2차원 배열로 인식\n",
    "- 차원 추가 메서드 : unsqueeze() eg. 맨 뒤에 길이가 1인 차원 추가 unsqueeze(-1)\n",
    "- 차원 제거 메서드 : squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d40cab8-d0f8-4eab-8e8a-083a523c0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(X)\n",
    "y = torch.FloatTensor(y).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e88590-31cb-4db5-9e90-1c52cc6007d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([506, 13]), torch.Size([506, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a7abc-61ce-451f-94f6-6d5582285340",
   "metadata": {},
   "source": [
    "### 표준화\n",
    "\n",
    "sklearn의 StandardSacler는 2차원 배열까지만 지원하기 때문에 3차원 이상의 데이터를 많이 다루는 딥러닝에서는 표준화를 직접 하는 것에 익숙해져야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d49f7f-0b82-43dc-b7b2-94f6e9ca7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - torch.mean(X))/torch.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730735d-eabb-4b3f-a579-0fb0388cf0dd",
   "metadata": {},
   "source": [
    "### 선형 회귀 모델 객체 생성\n",
    "\n",
    "(입력값, 출력값) = (특성 13개, 타겟 1개)\n",
    "\n",
    "- 활성화 함수 => 시그모이드 함수 => 로지스틱 회귀\n",
    "- 활성화 함수 없는 퍼셉트론 => 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babb7c4a-b244-49c0-a108-f7aa3528bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(13, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8e7fa-969e-4b19-98db-1e3cac119692",
   "metadata": {},
   "source": [
    "### 손실함수와 옵티마이저 객체 생성\n",
    "\n",
    "- MSE 손실 함수 -> criterion 변수 이름이 관례\n",
    "- 확률적 경사 하강법 옵티마이저 \n",
    "- parameters 메서드로 모델의 가중치를 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3c9484-3a98-4369-8117-030c117b9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca304a-8517-4f8d-8b73-41d6d79e88db",
   "metadata": {},
   "source": [
    "### 학습 함수 정의(에포크 1회) -> 오차(손실) 반환\n",
    "\n",
    "1. 손실 함수로 오차 계산(실제 타깃과 모델 예측 타깃의 차이)\n",
    "1. 미분으로 손실 함수 기울기 계산하여 가중치를 어떻게 수정해야 오차가 줄어드는지 파악\n",
    "1. 학습률만큼 가중치 수정\n",
    "\n",
    "한 번의 에포크는 주어진 모든 데이터 샘플을 이용해 한 번 학습하는 것을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4d407a-485c-43e2-aacc-055cc343f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, X, y):\n",
    "    \n",
    "    optimizer.zero_grad()           # 기울기 초기화\n",
    "    hypothesis = model(X)           # 모델 사용해 타깃 예측\n",
    "    loss = criterion(hypothesis, y) # 오차 계산\n",
    "    loss.backward()                 # 기울기 계산\n",
    "    optimizer.step()                # 경사하강법으로 가중치 수정\n",
    "    return loss.item()              # 현재 에포크 오차 반환 -> 파이썬 숫자형 자료구조로 변환(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea30fb-5c95-4892-a0eb-011719a80278",
   "metadata": {},
   "source": [
    "### 학습하며 오차 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c13528-aa2c-4536-822a-708ca362b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 115.4186\n",
      "epoch: 20, loss: 97.9571\n",
      "epoch: 30, loss: 88.5269\n",
      "epoch: 40, loss: 82.5146\n",
      "epoch: 50, loss: 78.6239\n",
      "epoch: 60, loss: 76.0548\n",
      "epoch: 70, loss: 74.3105\n",
      "epoch: 80, loss: 73.0827\n",
      "epoch: 90, loss: 72.1799\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100 # 학습 횟수\n",
    "for epoch in range(1, n_epochs):\n",
    "    loss = train(model, criterion, optimizer, X, y)\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch: {}, loss: {:.4f}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ec3fe-1e45-4bfe-bd02-9145cfa8c8a4",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀\n",
    "\n",
    "- 이진 크로스 엔트로피 손실 함수 사용\n",
    "- view 메서드는 넘파이의 reshape와 유사\n",
    "\n",
    "### 데이터 불러오기, 텐서 변환, 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66617dc0-5c06-43c7-8ddb-213cd66c1752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([569, 30]), torch.Size([569, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "\n",
    "X, y = dataset['data'], dataset['target']\n",
    "\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.FloatTensor(y).view(-1, 1)\n",
    "\n",
    "X = (X - torch.mean(X)) / torch.std(X)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4c3f4-207a-4072-b288-47546b752125",
   "metadata": {},
   "source": [
    "### nn.Sequential\n",
    "\n",
    "Sequential 클래스 생성자에 모델 객체와 활성화 함수 객체를 전달하면 두 객체를 차례대로 사용해 계산한 결과를 반환하는 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04b3b9b-e7e0-4932-8052-8d4452e8c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(30, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b5fd0-354a-4846-a4cb-e248b2277cd7",
   "metadata": {},
   "source": [
    "### 손실함수, 옵티마이저 정의 / 학습 함수 정의 / 학습\n",
    "\n",
    "이진 크로스 엔트로피(Binary Cross Entropy) 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e88d49d3-223f-4a72-92bd-33085a032417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 0.6046\n",
      "epoch: 20, loss: 0.5323\n",
      "epoch: 30, loss: 0.4806\n",
      "epoch: 40, loss: 0.4422\n",
      "epoch: 50, loss: 0.4126\n",
      "epoch: 60, loss: 0.3891\n",
      "epoch: 70, loss: 0.3701\n",
      "epoch: 80, loss: 0.3543\n",
      "epoch: 90, loss: 0.3411\n",
      "epoch: 100, loss: 0.3298\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "def train(model, criterion, optimizer, X, y):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    loss = criterion(hypothesis, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(model, criterion, optimizer, X, y)\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch: {}, loss: {:.4f}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aba859-c635-40a0-9598-b320de01da0c",
   "metadata": {},
   "source": [
    "### 타깃 예측 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "570bb5b4-5120-473a-a469-f94f867610c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 518.00\n"
     ]
    }
   ],
   "source": [
    "y_predicted = (model(X) >= 0.5).float()\n",
    "\n",
    "score = (y_predicted == y).float().sum()\n",
    "print('accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a8f8c-a74c-4201-b176-1a15d85158fb",
   "metadata": {},
   "source": [
    "## 클래스로 모델 정의\n",
    "\n",
    "- 파이토치에서 모델 클래스를 정의할 때는 nn.Module 클래스를 상속받아야 한다.\n",
    "- 모델 구조는 `__init__` 생성자 안에 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5c922-e14d-40e8-a9c2-29f0e06537b6",
   "metadata": {},
   "source": [
    "### 선형회귀 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3796a6b6-0279-4695-88fe-28514a74ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, num_features): # 생성자에서 모델 구조 정의\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "    \n",
    "    def forward(self, X): # 순전파 정의\n",
    "        out = self.linear(X)  # 생성자에서 만든 선형모델로 타깃 예측 및 반환\n",
    "        return out\n",
    "    \n",
    "model = LinearRegression(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fdbe6-dcb2-492c-a84f-8909e2e9004b",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7d45d67-6203-4703-83be-e59b97f7dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.linear(X)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "model = LogisticRegression(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48904531-d595-4d27-a09c-9357d4220927",
   "metadata": {},
   "source": [
    "## 배치(batch) 학습\n",
    "\n",
    "전체 데이터에 대한 추론과 오차 역전파를 한 번에 실행하기에는 컴퓨터 메모리 한계가 존재한다. 이 문제를 해결하기 위해 데이터를 나눠서 처리하는 배치라는 개념이 등장한다. 배치란 한 번의 에포크를 수행할 때 처리하는 샘플의 개수를 의미한다. 전체 데이터 샘플을 지정된 배치 크기로 나눠서 학습하고 마지막 배치 학습이 끝나면 한 번의 에포크가 수행된 것이다.\n",
    "\n",
    "배치 학습을 위해 DataLoader 클래스 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c7edd-56fe-47dd-93fd-eb6710304060",
   "metadata": {},
   "source": [
    "### 데이터 로드, 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b370184-f4fa-4049-afe3-c54bb7c28896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset['data'], dataset['target']\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.FloatTensor(y).view(-1,1)\n",
    "\n",
    "X = (X - torch.mean(X)) / torch.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22e9b9-94b7-4887-9211-ff7a57faf741",
   "metadata": {},
   "source": [
    "### 배치 학습 구현 위해 데이터와 타깃 합치기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a08f83b-f979-4355-a5cc-0581200daf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터와 타깃을 묶어 텐서 데이터셋 생성\n",
    "dset = TensorDataset(X, y)\n",
    "\n",
    "# 한 번에 256개의 데이터 샘플을 배치로 사용하는 데이터로더 생성\n",
    "loader = DataLoader(dset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a0600-f3e6-4869-a177-73b80a828297",
   "metadata": {},
   "source": [
    "### 신경망 모델 클래스 정의 / 학습 함수 정의 / 학습 / 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67a0771b-3893-4fa9-8c05-1bed31464e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 0.5338\n",
      "epoch: 20, loss: 0.3442\n",
      "epoch: 30, loss: 0.2628\n",
      "epoch: 40, loss: 0.2579\n",
      "epoch: 50, loss: 0.2114\n",
      "epoch: 60, loss: 0.1996\n",
      "epoch: 70, loss: 0.2114\n",
      "epoch: 80, loss: 0.2221\n",
      "epoch: 90, loss: 0.2065\n",
      "epoch: 100, loss: 0.2161\n",
      "accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_features, 4) # 은닉층 노드 4개 생성\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.linear1(X)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNetwork(30) \n",
    "criterion = nn.BCELoss()  # 이진 크로스 엔트로피\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train(model, criterion, optimizer, loader):\n",
    "    epoch_loss = 0    # 현재 에포크의 오차 저장 변수\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X_batch)\n",
    "        loss = criterion(hypothesis, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() # 현재 배치의 오차 누적\n",
    "        \n",
    "    return epoch_loss / len(loader) # 현대 에포크의 오차 평균 반환\n",
    "\n",
    "# 모델 학습\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(model, criterion, optimizer, loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch: {}, loss: {:.4f}'.format(epoch, loss))\n",
    "        \n",
    "# 학습된 모델로 타깃 예측\n",
    "y_predicted = (model(X) >= 0.5).float()\n",
    "\n",
    "# 정확도 계산\n",
    "score_trained_model = (y_predicted == y).float().mean()\n",
    "print('accuracy: {:.2f}'.format(score_trained_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f421031-c8c1-43ba-a000-47d410a5edc3",
   "metadata": {},
   "source": [
    "### 모델 저장\n",
    "\n",
    "- model.state_dict() 모델 정보를 가진 딕셔너리에 접근\n",
    "- torch.save() 모델 저장 .pt 또는 .pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b788f8de-f15b-4084-a2d2-cae3005ca86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'breast_cancer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83805b2-4715-49ed-a129-3c02cb66707d",
   "metadata": {},
   "source": [
    "### 모델 로드, 예측\n",
    "\n",
    "- torch.load() 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "698ba073-50b7-404e-92b4-35e4e794bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of loaded model: 0.92\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델의 객체 생성\n",
    "loaded_model = NeuralNetwork(30)\n",
    "\n",
    "# 저장한 파일에서 가중치 불러와 복원\n",
    "loaded_model.load_state_dict(torch.load('breast_cancer.pt'))\n",
    "\n",
    "# 타깃 예측\n",
    "y_pred2 = (loaded_model(X) >= 0.5).float()\n",
    "\n",
    "score_loaded_model = (y_pred2 == y).float().mean()\n",
    "print('accuracy of loaded model: {:.2f}'.format(score_loaded_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
