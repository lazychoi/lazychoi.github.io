{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"독산 미니 프로젝트 - 유사 도서 추천 시스템, 챗봇\"\n",
    "date: 2022/10/27\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사 도서 추천 시스템\n",
    "\n",
    "[앱 시연](https://nammtaeehyeonn-reco-recommend-utwo4b.streamlit.app/){target=_blank}\n",
    "\n",
    "- 책 제목을 입력하면 내용이 유사한 도서 5권 추천하는 앱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 행렬의 크기(shape) : (2999, 30145)\n",
      "코사인 유사도 연산 결과 : (2999, 2999)\n"
     ]
    }
   ],
   "source": [
    "# calc_similarity.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('data/book.csv')\n",
    "\n",
    "# 제 컴퓨터로는 데이터 개수를 줄여야 돌아갑니다...\n",
    "df1 = df.head(3000)\n",
    "\n",
    "description = df1['description']\n",
    "\n",
    "# TF - IDF 기법으로 벡터화\n",
    "# 20000개의 줄거리는 약 80000개의 단어로 이루어져있다. (전처리 완료돼서 많이 줄었다.)\n",
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf_matrix = tf_idf.fit_transform(description)\n",
    "print('TF-IDF 행렬의 크기(shape) :',tf_idf_matrix.shape)\n",
    "\n",
    "# 전체 유사도 계산\n",
    "cos_sim = cosine_similarity(tf_idf_matrix, tf_idf_matrix)\n",
    "print('코사인 유사도 연산 결과 :',cos_sim.shape)\n",
    "\n",
    "np.save('data/sim.npy', cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend.py\n",
    "\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Recommend(title, cos_sim):\n",
    "    \n",
    "    title_idx = dict(zip(df['Title'], df.index))\n",
    "    # 도서 제목 입력하면 인덱스를 리턴\n",
    "    idx = title_idx[title]\n",
    "\n",
    "    # 도서 줄거리 유사도 전부 가져오기\n",
    "    sim = list(enumerate(cos_sim[idx]))\n",
    "\n",
    "    # 유사도에 따라 정렬하기\n",
    "    sim = sorted(sim, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 유사도 탐5 가져오기\n",
    "    sim = sim[1:6]\n",
    "\n",
    "    # 유사도 탑5 인덱스 가져오기\n",
    "    rec_idx = [idx[0] for idx in sim]\n",
    "\n",
    "    # 유사도 탑10 제목 가져오기\n",
    "    # print(sim)                       # (인덱스, 유사도)\n",
    "    return df.iloc[rec_idx]\n",
    "\n",
    "df = pd.read_csv('book.csv')\n",
    "cos_sim = np.load('sim.npy')\n",
    "\n",
    "# streamlit\n",
    "st.title('독서는 마음의 양식')\n",
    "\n",
    "title = st.text_input(\"책 제목을 입력해주세요\")\n",
    "\n",
    "if title:\n",
    "    ans = Recommend(title, cos_sim)\n",
    "    ans.reset_index(drop = True, inplace = True)\n",
    "     \n",
    "    for i in range(5):\n",
    "        st.subheader(ans.Title[i])    # 제목\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.image(ans.image[i], width = 200)  # 책 표지\n",
    "        with col2:\n",
    "            st.write(ans.description[i][:400] + \"   ...\")  # 책 내용\n",
    "        st.header(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### streamlit에서 앱 작동\n",
    "\n",
    "<img src='images/Screenshot2022-11-284.44.59.png' width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마르코프 체인을 이용한 챗봇 엔진 만들기\n",
    "\n",
    "\"파이썬을 이용한 머신러닝, 딥러닝 실전 개발 입문(쿠지라 히코우즈쿠에)\"의 6장에서 소개한 코드를 변경해서 구현했습니다. 마르코프 체인이 무엇인지에 대해서 설명할 능력은 없습니다. 궁금하면 여기☞ [유튜브 설명](https://www.youtube.com/watch?v=Yh62wN2kMkA)를 눌러 보세요. \n",
    "\n",
    "제목에 마르코프 체인을 이용해서 챗봇 엔진을 만들었다고 했지만, 코드를 보면 확률을 사용하지 않고 랜덤하게 다음 단어를 선택했음을 알 수 있습니다. 구체적으로 이 책에서 제공한 코드가 어떻게 작동하는지 아래 2줄의 대화 데이터로 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "이번에 캘리포니아에 산불 난 거 보셨어요?\n",
    "네 봤어요 ㅜㅜ 일주일 넘게 진압하고 있다는데 걱정이네요...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "konlpy에서 제공하는 Okt 클래스를 이용해 형태소 분석을 했습니다. 다음은 형태소 분석한 데이터입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 'Foreign'), ('이번', 'Noun'), ('에', 'Josa'), ('캘리포니아', 'Noun'), ('에', 'Josa'), ('산불', 'Noun'), ('난', 'Noun'), ('거', 'Noun'), ('보셨어요', 'Verb'), ('?', 'Punctuation'), ('\\n', 'Foreign'), ('네', 'Noun'), ('봤어요', 'Verb'), ('ㅜㅜ', 'KoreanParticle'), ('일주일', 'Noun'), ('넘게', 'Verb'), ('진압', 'Noun'), ('하고', 'Josa'), ('있다는데', 'Adjective'), ('걱정', 'Noun'), ('이네', 'Josa'), ('요', 'Noun'), ('...', 'Punctuation'), ('\\n', 'Foreign')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "morph_list = okt.pos(text)\n",
    "print(morph_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석한 데이터 중에서 품사명은 제외하고 형태소만을 리스트로 추린 후 3 단어씩 묶었습니다. 이렇게 묶은 이유는 어떤 단어의 뒤에 오는 단어를 2개까지 확보하기 위해서입니다. 특정 단어 뒤에 올만한 단어를 2개까지 묶는 겁니다. '@'는 문장의 처음을 표시하는 기호입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '이번', '에', '캘리포니아', '에', '산불', '난', '거', '보셨어요', '\\n', '네', '봤어요', 'ㅜㅜ', '일주일', '넘게', '진압', '하고', '있다는데', '걱정', '이네', '요', '\\n']\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for morph in morph_list:\n",
    "    # 마침표를 제외한 모든 구두점 제외\n",
    "    if not morph[1] in ['Punctuation']:\n",
    "        word_list.append(morph[0])\n",
    "    if morph[0] == '.':\n",
    "        word_list.append(morph[0])\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터를 만드는 마지막 단계입니다. 위에서 만든 리스트를 딕셔너리 자료형으로 바꿉니다. 이 딕셔너리가 문장을 만들 때 어떻게 사용되는지는 다음과 같습니다. \n",
    "\n",
    "예를 들어, \n",
    "1. 사용자가 '이번(w1)'이라는 단어를 입력하면 '이번'이라는 키에 해당하는 값을 찾습니다. (①)\n",
    "2. 아래 딕셔너리에는 값이 '에(w2)' 하나뿐입니다. 따라서 '이번'키의 값인 '에'와 '에'키의 값인 '캘리포니아'를 반환합니다. \n",
    "3. 다음은 부모로 '에'키와 '캘니포니아'키를 지닌 키를 찾습니다. 여기서는 '에(w3)'네요. (②)\n",
    "4. 지금까지 만들어진 문장은 '이번 에 캘리포니아 에'입니다.\n",
    "5. 이어서 부모 키가 '캘리포니아'키와 '에'키인 단어를 찾습니다. '산불'이네요.(③)\n",
    "6. 그러면 '이번 에 캘리포니아 에 산불'이라는 문장이 만들어집니다.\n",
    "7. 이런 과정을 반복해 문장을 만들고, 값으로 마침표나 물음표 같은 문장부호, 또는 공백이 나오면 문장 생성을 종료합니다.\n",
    "\n",
    "여기서는 설명을 위해 문장 2개로 만든 사전의 일부만 사용해서 자료와 똑같은 문장이 만들어졌지만, 많은 문장을 사용하면 하나의 단어키에 해당하는 값이 여러 개가 되고, 그 중에 하나를 랜덤하게 추출하는 것으로 코드가 작성되어 있어 데이터와 다른 문장이 생성됩니다. 랜덤하게 값에 해당하는 형태소를 가져오기 때문에 마르코프 체인의 확률은 구현되지 않은 것 같습니다. 그래서인지 문법이나 뜻이 어색한 문장이 종종 만들어집니다.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"@\": {\n",
    "    \"이번\": {\n",
    "      \"에\": 1\n",
    "    },\n",
    "    \"네\": {\n",
    "      \"봤어요\": 1\n",
    "    }\n",
    "  },\n",
    "  \"이번\": {     <------- ①\n",
    "    \"에\": {\n",
    "      \"캘리포니아\": 1\n",
    "    }\n",
    "  },\n",
    "  \"에\": {\n",
    "    \"캘리포니아\": {\n",
    "      \"에\": 1  <------- ②\n",
    "    },\n",
    "    \"산불\": {\n",
    "      \"난\": 1\n",
    "    }\n",
    "  },\n",
    "  \"캘리포니아\": {\n",
    "    \"에\": {\n",
    "      \"산불\": 1  <------- ③\n",
    "    }\n",
    "(이하 생략)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@': {'이번': {'에': 1}}, '이번': {'에': {'캘리포니아': 1}}, '에': {'캘리포니아': {'에': 1}, '산불': {'난': 1}}, '캘리포니아': {'에': {'산불': 1}}, '산불': {'난': {'거': 1}}, '난': {'거': {'보셨어요': 1}}, '거': {'보셨어요': {'네': 1}}, '보셨어요': {'네': {'봤어요': 1}}, '네': {'봤어요': {'ㅜㅜ': 1}}, '봤어요': {'ㅜㅜ': {'일주일': 1}}, 'ㅜㅜ': {'일주일': {'넘게': 1}}, '일주일': {'넘게': {'진압': 1}}, '넘게': {'진압': {'하고': 1}}, '진압': {'하고': {'있다는데': 1}}, '하고': {'있다는데': {'걱정': 1}}, '있다는데': {'걱정': {'이네': 1}}, '걱정': {'이네': {'요': 1}}}\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "\n",
    "tmp = ['@']\n",
    "for i, word in enumerate(word_list):\n",
    "    if word == '' or word =='\\r\\n' or word == '\\n': continue\n",
    "\n",
    "    tmp.append(word)\n",
    "    if len(tmp) < 3: continue      # 3개 단어씩 묶기\n",
    "    if len(tmp) > 3: tmp = tmp[1:] # tmp 리스트 길이가 3을 넘어가면 첫 번째 요소 제외\n",
    "    # print(tmp)\n",
    "    # sys.exit(0)\n",
    "\n",
    "    w1, w2, w3 = tmp \n",
    "    if not w1 in dic: dic[w1] = {}   # w1 단어가 사전에 없으면 등록\n",
    "    if not w2 in dic[w1]: dic[w1][w2] = {}  # w2 단어가 w1 블록 안에 없으면 등록\n",
    "    if not w3 in dic[w1][w2]: dic[w1][w2][w3] = 0 # w3 단어가 w2 블록 안에 없으면 등록 후 0으로 초기화\n",
    "    dic[w1][w2][w3] += 1   # w3 단어가 나왔기 때문에 +1\n",
    "    \n",
    "    if word == '.' or word == '?':  # 마침표를 만나면 tmp 초기화\n",
    "        tmp = ['@']\n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@\": {\n",
      "    \"이번\": {\n",
      "      \"에\": 1\n",
      "    }\n",
      "  },\n",
      "  \"이번\": {\n",
      "    \"에\": {\n",
      "      \"캘리포니아\": 1\n",
      "    }\n",
      "  },\n",
      "  \"에\": {\n",
      "    \"캘리포니아\": {\n",
      "      \"에\": 1\n",
      "    },\n",
      "    \"산불\": {\n",
      "      \"난\": 1\n",
      "    }\n",
      "  },\n",
      "  \"캘리포니아\": {\n",
      "    \"에\": {\n",
      "      \"산불\": 1\n",
      "    }\n",
      "  },\n",
      "  \"산불\": {\n",
      "    \"난\": {\n",
      "      \"거\": 1\n",
      "    }\n",
      "  },\n",
      "  \"난\": {\n",
      "    \"거\": {\n",
      "      \"보셨어요\": 1\n",
      "    }\n",
      "  },\n",
      "  \"거\": {\n",
      "    \"보셨어요\": {\n",
      "      \"네\": 1\n",
      "    }\n",
      "  },\n",
      "  \"보셨어요\": {\n",
      "    \"네\": {\n",
      "      \"봤어요\": 1\n",
      "    }\n",
      "  },\n",
      "  \"네\": {\n",
      "    \"봤어요\": {\n",
      "      \"ㅜㅜ\": 1\n",
      "    }\n",
      "  },\n",
      "  \"봤어요\": {\n",
      "    \"ㅜㅜ\": {\n",
      "      \"일주일\": 1\n",
      "    }\n",
      "  },\n",
      "  \"ㅜㅜ\": {\n",
      "    \"일주일\": {\n",
      "      \"넘게\": 1\n",
      "    }\n",
      "  },\n",
      "  \"일주일\": {\n",
      "    \"넘게\": {\n",
      "      \"진압\": 1\n",
      "    }\n",
      "  },\n",
      "  \"넘게\": {\n",
      "    \"진압\": {\n",
      "      \"하고\": 1\n",
      "    }\n",
      "  },\n",
      "  \"진압\": {\n",
      "    \"하고\": {\n",
      "      \"있다는데\": 1\n",
      "    }\n",
      "  },\n",
      "  \"하고\": {\n",
      "    \"있다는데\": {\n",
      "      \"걱정\": 1\n",
      "    }\n",
      "  },\n",
      "  \"있다는데\": {\n",
      "    \"걱정\": {\n",
      "      \"이네\": 1\n",
      "    }\n",
      "  },\n",
      "  \"걱정\": {\n",
      "    \"이네\": {\n",
      "      \"요\": 1\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "pretty = json.dumps(dic, indent=2, ensure_ascii=False)\n",
    "print(pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터\n",
    "\n",
    "위의 책에서는 소설 토지로 사전을 만들었지만, 챗봇에는 어울리지 않아 아래 사이트에서 제공하는 SNS 대화 텍스트 파일을 이용했습니다.\n",
    "\n",
    "<img width=\"600px\" src=\"https://user-images.githubusercontent.com/8787919/203877544-822fcb64-55ce-4521-a3ac-ba6f597d309b.png\">\n",
    "\n",
    "데이터 파일만 98,654개이고 하나의 파일은 대략 20에서 30줄 정도의 문장으로 이루어져 있습니다. 이것을 사전으로 변환하는데 제 M1 맥북에어로 이틀을 꼬박 돌렸는데도 절반도 하지 못했고, 메모리가 부족해서인지 속도가 점점 더 느려져 결국 중단했습니다. 그래서 10,000개 파일만 변환했습니다. \n",
    "\n",
    "## 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번 에 캘리포니아 에 산불 난 거 보셨어요 네 봤어요 ㅜㅜ 일주일 넘게 진압 하고 있다는데 걱정 이네 요 \n",
      "산불 난 거 보셨어요 네 봤어요 ㅜㅜ 일주일 넘게 진압 하고 있다는데 걱정 이네 요 \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def word_choice(select):\n",
    "    keys = select.keys()\n",
    "    return random.choice(list(keys))\n",
    "\n",
    "input_list = ['@', '산불', '안녕']\n",
    "\n",
    "for input_text in input_list:\n",
    "    if not input_text in dic: break\n",
    "    sentence = []\n",
    "    if input_text != '@': sentence.append(input_text)\n",
    "    top = dic[input_text]\n",
    "    w1 = word_choice(top)\n",
    "    w2 = word_choice(top[w1])\n",
    "    sentence.append(w1)\n",
    "    sentence.append(w2)\n",
    "    while True:\n",
    "        if w1 in dic and w2 in dic[w1]:\n",
    "            w3 = word_choice(dic[w1][w2])\n",
    "        else: w3 = ''\n",
    "        sentence.append(w3)\n",
    "        if w3 == '\\n' or w3 == '' or w3 == '.' or w3 == '?': break\n",
    "        w1, w2 = w2, w3\n",
    "    result = ' '.join(sentence)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 이용 사용자가 입력한 단어와 유사한 단어로 문장 생성 \n",
    "\n",
    "위 코드는 사용자가 입력한 단어를 포함해 문장을 만듭니다. 마치 앵무새처럼 말이죠. 그래서 같은 단어를 사용하는 대신에 의미가 유사한 단어를 이용하는 것으로 바꿔보았습니다. \n",
    "\n",
    "사전 파일을 만들 때 사용한 데이터에서 명사를 추출하여 word2vec으로 학습시켰습니다. 사용자가 입력한 단어 중에서 명사를 뽑아 word2vec으로 유사한 단어를 찾은 뒤 그 단어를 사전에 키로 넣어 문장을 만들었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus count: 22\n",
      "corpus total words: 44\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences=word_list, vector_size=200, window=4, hs=1, min_count=2, sg=1)\n",
    "print('corpus count:', w2v.corpus_count)\n",
    "print('corpus total words:', w2v.corpus_total_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "6fb0ca7c3bd8bc698549aa02a9df33484cafacf9c2c862385157fe6bff41776c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
